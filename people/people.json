[
{
    "name" : "Alain Destexhe",
    "last_name": "Destexhe",
    "affiliation": "CNRS, European Institute of Theoretical Neuroscience, Paris, France",
    "title" : "Awake perception is associated with dedicated neuronal assemblies in cerebral cortex ",
    "url" : "https://neuropsi.cnrs.fr/annuaire/alain-destexhe/",
    "image_src": "assets/img/alain_destehxe.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "0945",
     "abstract": "Neural activity in the sensory cortex combines stimulus responses and ongoing activity, but it remains unclear whether these reflect the same underlying dynamics or separate processes. In the present study, we show in mice that, during wakefulness, the neuronal assemblies evoked by sounds in the auditory cortex and thalamus are specific to the stimulus and distinct from the assemblies observed in ongoing activity. By contrast, under three different anesthetics, evoked assemblies are indistinguishable from ongoing assemblies in the cortex. However, they remain distinct in the thalamus. A strong remapping of sensory responses accompanies this dynamic state change produced by anesthesia. Together, these results show that the awake cortex engages dedicated neuronal assemblies in response to sensory inputs, which we suggest is a network correlate of sensory perception (Filipchuk et al., Nature Neuroscience 25: 1327-1338, 2022)."
},
{
    "name" : "Richard Naud",
    "last_name": "Naud",
    "affiliation": "University of Ottawa",
    "title" : "New biological solutions to the credit assignment problem",
    "url" : "https://med.uottawa.ca/cellular-molecular/people/naud-richard",
    "image_src": "assets/img/richard_naud.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "1600",
     "abstract": "Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well established that it depends on pre- and postsynaptic activity. How can such local learning give rise to meaningful decisions after being used by millions of neurons? Particularly, when the neurons receiving raw data are many synapses apart from the neurons making the decisions, how can the credit of a right/wrong decision be communicated to all the synapses that have taken a part in it? This is the credit assignment problem. AI has many algorithms to solve it, but these are impossible to implement by the brain as such. In this presentation, I am going to survey the recent theories and data that allowing us to understand how the brain coordinates plasticity by approximations of AI algorithms"
},    
{
    "name" : "Stefano Panzeri",
    "last_name": "Panzeri",
    "affiliation": "UKE Hamburg, Germany",
    "title" : "How activity correlations between neurons shape encoding and readout of sensory information",
    "url" : "https://www.uke.de/english/departments-institutes/institutes/department-of-excellence-for-neural-information-processing/team/index.html",
    "image_src": "assets/img/panzeri_stefano.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "1100",
     "abstract": "The collective activity of a population of neurons is critical for many brain functions. A fundamental question is how activity correlations between neurons affect how neural populations process information. We present a theory, built on the analyses of simultaneous recordings of activity of populations on neurons either in sensory or posterior parietal cortical areas, of how such correlations serve multiple functions performed by neural populations, including shaping the encoding of information in population codes, generating codes across multiple timescales, and facilitating information transmission to and readout by downstream brain areas to guide behavior. Here, we review this theory and we further present ideas on how to combine large-scale simultaneous recordings of neural populations, computational models, analyses of behavior, optogenetics, and anatomy to unravel how the structure of correlations might be optimized to serve multiple functions.     "
},
{
    "name" : "Upinder Bhalla",
    "last_name": "Bhalla",
    "affiliation": "National Center for Biological Sciences, Bangalore, India",
    "title" : "Seeing what matters: how single neurons extract patterns amidst noise",
    "url" : "https://www.ncbs.res.in/faculty/bhalla",
    "image_src": "assets/img/upi_bhalla.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "1100",
     "abstract": "Neurons are remarkably fast, efficient and versatile computing units. To first order, a single neuron can be simulated by a rather substantial neural network model, or by a large system of partial differential equations. What computations can the real neuron do with these complex dynamics? I'll discuss two examples, one like the prediction of where a cricket ball will fly, and the other related to the ''what was that?'' moment when we hear a  repeated sound amidst background noise. One of the uniquely powerful capabilities of neurons is their ability to extract signals from complex patterns in space and time, against a background of uncorrelated neural activity. The trajectory prediction problem can be framed as sequential pattern recognition. I'll discuss how sequential pattern recognition is implemented in neurons, and how this operation is equivalent to the functioning of a convolutional neural network. I'll then take on the ''what was that?'' problem of recognizing unique repeated inputs. I'll discuss some preliminary data which suggests that excitatory-inhibitory (EI) balance is asymmetric with respect to sequences of patterns. This means that certain spatiotemporal input sequences can 'escape' EI balance and cause the neuron to become more active. Overall, both these computations, take advantage of the intricate biophysics and biochemistry of the neuron to perform real-time pattern recognition with extraordinary energy efficiency."
},
{
"name" : "Vatsala Thirumalai",
"last_name": "Thirumalai",
"affiliation": "National Center for Biological Sciences, Bangalore, India",
"title" : "Expectation and Error Encoding in Cerebellar Purkinje Neurons",
"url" : "https://www.ncbs.res.in/faculty/vatsala",
"image_src": "assets/img/vatsala.jpeg",
"field": "Neuromodulation",
"date": "16.12.2022",
"day": "Friday",
"time": "0945",
 "abstract": "The cerebellum is known to be involved in minimizing error by participating in the acquisition and updating of internal models of the world. However, several key features of how this is accomplished is not yet clear. By using larval zebrafish, we could monitor the population activity of cerebellar Purkinje neurons while the larvae are engaged in a stereotyped optomotor behavior. We acclimatized the fish to predictable stimuli and then challenged them with a novel stimulus. Purkinje neuron activity remained modest during acclimitizing trials but reported an enhanced error signal during the novel stimulus. The error signal increased in amplitude with increased number of repetitions of the acclimitizing stimulus, reaching saturation at 16 repetitions. I will discuss these results in detail and propose that Purkinje neurons signal the need to update the internal model when there are mismatches between expectation and reality by way of their error signals. "
},
{
"name" : "Rishikesh Narayanan",
"last_name": "Narayanan",
"affiliation": "Indian Institute of Sciences, Bangalore, India",
"title" : "Efficient information coding and degeneracy in the nervous system",
"url" : "http://mbu.iisc.ac.in/~rngrp/",
"image_src": "assets/img/Rishi.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Thursday",
"time": "1145",
"abstract": "Efficient information coding (EIC) is a universal biological framework rooted in the fundamental principle that system responses should match their natural stimulus statistics for maximizing environmental information. In this talk, the following arguments about EIC will be discussed: (1) EIC is ubiquitous: spans species, systems, and scales. This contrasts prevailing assumptions that EIC is limited to sensory systems neuroscience. Illustrative examples spanning all scales — molecular to behavioral — across species will be presented to emphasize the ubiquitous nature of EIC. (2) )Biological complexity and associated degeneracy are effective substrates to achieve EIC in conjunction with system stability. Complexity in a system is characterized by several functionally segregated subsystems that show a high degree of functional integration when they interact with each other. Degeneracy, the ability of structurally different components to yield the same functional outcome, provides an efficacious substrate for implementing stable EIC in biological systems."
},
{
"name" : "Vijay Balasubramnium",
"last_name": "Balasubramnium",
"affiliation": "University of Pennsylvania, PA, USA",
"title" : "Occam's Razor in inference by humans and machines",
"url" : "https://live-sas-physics.pantheon.sas.upenn.edu/people/standing-faculty/vijay-balasubramanian",
"image_src": "assets/img/vijay_bala.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Thursday",
"time": "1400",
"abstract": "Occam's Razor is the principle that, all else being equal, simpler explanations for a set of observations should be preferred over complex ones.  This idea, derived from a famous dictum of the 13th century Scholastic philosopher William of Ockham, has a quantitative realization in modern mathematical theories of statistical model selection.   Experiments have also suggested that it plays a role in human perception and decision making.  I will describe the Bayesian theory of Occam's Razor, and discuss how it penalizes 'complexity' understood in terms of the geometry of models seen as manifolds in the space of probability distributions.  Then I will present data from experiments with humans and artificial neural networks trained to perform decision tasks.  I will show that both humans and machines demonstrate an aversion to complex explanations, with simplicity biases quantitatively explained by the Bayesian theory.   These biases persist in humans, but not in machines, even in tasks where they are maladaptive and lower overall performance. Taken together, our results imply that principled notions of statistical model complexity have direct, quantitative relevance to human and machine decision-making. Our results also establish a new understanding of the computational foundations, and behavioral benefits, of our predilection for inferring simplicity in the latent properties of our complex world."
},
{
"name" : "Yasser Roudi",
"last_name": "Roudi",
"affiliation": "Kavli Institute for Systems Neuroscience, Trondheim, Norway",
"title" : "Quantifying relevance in learning and inference",
"url" : "https://www.ntnu.edu/employees/yasser.roudi",
"image_src": "assets/img/yasser_roudi.jpeg",
"field": "Neural Coding",
"date": "16.12.2022",
"day": "Friday",
"time": "1400",
"abstract": "Learning is a distinctive feature of intelligent behaviour. High-throughput experimental data and Big Data promise to open new windows on complex systems such as cells, the brain or our societies. Yet, the puzzling success of Artificial Intelligence and Machine Learning shows that we still have a poor conceptual understanding of learning. These applications push statistical inference into uncharted territories where data is highdimensional and scarce, and prior information on 'true' models is scant if not totally absent. Here we review recent progress on understanding learning, based on the notion of 'relevance'. The relevance, as we define it here, quantifies the amount of information that a dataset or the internal representation of a learning machine contains on the generative model of the data. This allows us to define maximally informative samples, on one hand, and optimal learning machines on the other. These are ideal limits of samples and of machines, that contain the maximal amount of information about the unknown generative process, at a given resolution (or level of compression). Both ideal limits exhibit critical features in the statistical sense: Maximally informative samples are characterised by a power-law frequency distribution (statistical criticality) and optimal learning machines by an anomalously large susceptibility. The trade-off between resolution (i.e. compression) and relevance distinguishes the regime of noisy representations from that of lossy compression. These are separated by a special point characterised by Zipf’s law statistics. This identifies samples obeying Zipf’s law as the most compressed loss-less representations that are optimal in the sense of maximal relevance. Criticality in optimal learning machines manifests in an exponential degeneracy of energy levels, that leads to unusual thermodynamic properties. This distinctive feature is consistent with the invariance of the classification under coarse graining of the output, which is a desirable property of learning machines. This theoretical framework is corroborated by empirical analysis showing (i) how the concept of relevance can be useful to identify relevant variables in high-dimensional inference and (ii) that widely used machine learning architectures approach reasonably well the ideal limit of optimal learning machines, within the limits of the data with which they are trained."
},
{
"name" : "Roshan Cools",
"last_name": "Cools",
"affiliation": "Donders Institute for Brain, Cognition and Behaviour, Centre for Cognitive Neuroimaging. The Netherlands",
"title" : "Chemistry of the adaptive mind. Lessons from dopamine?",
"url" : "http://www.roshancools.com/",
"image_src": "assets/img/roshan_cools.jpg",
"field": "Neuromodulation",
"date": "16.12.2022",
"day": "Friday",
"time": "1145",
"abstract": "The human brain faces a variety of computational dilemmas, including the flexibility/stability, the speed/accuracy and the labor/leisure tradeoff. Given its role in reinforcement learning and motivation, striatal dopamine is particularly well suited to dynamically regulate these computational tradeoffs depending on constantly changing task demands. To illustrate this, I will discuss evidence from studies on learning, motivation and cognitive control in human volunteers, using chemical PET imaging, psychopharmacology, and/or fMRI. These studies also begin to elucidate the mechanisms underlying the huge variability in dopaminergic drug effects across different individuals and across different task contexts"
},
{
"name" : "Dinos Meletis",
"last_name": "Meletis",
"affiliation": "Karolinska Institute, Sweden",
"title" : "The organization and function of circuits shaping motivated behaviors",
"url" : "https://ki.se/en/neuro/meletis-laboratory",
"image_src": "assets/img/Dinos_Meletis.jpeg",
"field": "Neuromodulation",
"date": "16.12.2022",
"day": "Friday",
"time": "1100",
"abstract": "The organization of brain circuits reflects a biological template for producing diverse and complex behaviors. To address the connection between organization and function, we have investigated the molecular and connectivity principles of circuits at the whole-brain scale to establish the anatomy and circuit organization based on unbiased definitions. We have used this knowledge to study the role of neuron types in motivated behaviors, focusing on the role of neuron types in the basal ganglia and the hypothalamic-habenula pathway. I will discuss progress in using multimodal classification of circuits and neurons to define how negative signals induce avoidance and stress states."
},
{
"name" : "Katharina Glomb",
"last_name": "Glomb",
"affiliation": "Berlin Institute of Health at Charité (BIH)",
"title" : "Towards quality control for harmonic modes",
"url" : "https://wp.unil.ch/connectomics/katharina-glomb/",
"image_src": "assets/img/Katharina_Glomb.jpeg",
"field": "Graphs",
"date": "16.12.2022",
"day": "Friday",
"time": "1400",
"abstract": "Basic graph signal processing (GSP) techniques can be used to transform time series obtained through neuroimaging (be it M/EEG, fMRI, or others) into the graph spectral domain. The necessary basis functions - or 'harmonic modes' in network neuroscience - are obtained from functional or structural connectivity matrices that encode network organization in the brain. This procedure involves a number of user-defined parameter choices, namely the type of Laplacian, the number of nearest neighbors in the graph, and the number of dimensions of the reduced space, all of which have an impact on the properties of the harmonic modes. If one applies GSP for the purposes of dimensionality reduction, it is relatively easy to choose these parameters, as one wants to achieve optimal compression. However, it is less clear what properties harmonic modes should have to meaningfully reflect the underlying brain connectivity. Quantitative criteria as to how to set these parameters - or indeed a systematic understanding of how parameter choices impact harmonic modes - are missing thus far, posing a serious limitation for interpreting harmonic modes in terms of their functional significance. In my talk, I will address recent ideas of how to tackle this limitation, looking at embedding and reconstruction performance as well as discussing graph properties that can be derived from the harmonic modes themselves. "
},
{
"name" : "Maria Giulia Preti",
"last_name": "Preti",
"affiliation": "Center for Biomedical Imaging, EPFL, Switzerland",
"title" : "Graph signal processing for neurogimaging to identify unique structure-function coupling signatures ",
"url" : "https://people.epfl.ch/maria.preti/?lang=en",
"image_src": "assets/img/maria_preti.jpeg",
"field": "Signal Processing",
"date": "16.12.2022",
"day": "Friday",
"time": "1445",
"abstract": "The relation between functional activity and the underlying neural architecture in the brain is complex and remains largely unexplored. In this context, graph signal processing (GSP) represents a novel framework allowing to link functional activity signals, e.g., from functional magnetic resonance imaging (fMRI), and the structural connections underneath (assessed with diffusion MRI) in a non-trivial way. Functional activity patterns are represented as graph signals, defined on top of the structural connectome graph, and graph spectral filtering can be used to distinguish the components of the functional activity that are more or less smooth on the graph; i.e., coupled or decoupled from brain structure, respectively. Here, we investigate and compare structure-function coupling assessed at different temporal scales and with different modalities; i.e., resting-state fMRI and magnetoencephalography (MEG). In particular, we investigate the capability of structure-function coupling to identify individuals, representing a unique signature of their brains. We conclude that structure-function coupling performs very accurately for individual fingerprinting, with different brain patterns contributing at different time-scales."
},
{
"name" : "Nicolas Farrugia",
"last_name": "Farrugia",
"affiliation": " IMT Atlantique, France",
"title" : "Interpreting brain activity through connectivity using graph signal processing",
"url" : "https://nicofarr.github.io/",
"image_src": "assets/img/nico_juin.jpeg",
"field": "AI",
"date": "16.12.2022",
"day": "Friday",
"time": "1600",
"abstract": "The application of graph theory to model the complex structure and function of the brain has shed new light on its organization, prompting the emergence of network neuroscience. Despite the tremendous progress that has been achieved in this field, still relatively few methods exploit the topology of brain networks to analyze brain activity. Recent attempts in this direction have leveraged on the one hand graph spectral analysis (to decompose brain connectivity into eigenmodes or gradients) and graph signal processing (to decompose brain activity “coupled to” an underlying network in graph Fourier modes). In this talk, we will describe two ongoing works that attempt at integrating knowledge from brain connectivity in order to decode and interpret brain activity. In the first contribution, we use functional connectivity graphs to define spectral convolution operators in a deep residual network trained on task decoding. We show how parameter pruning can be used to select the most important connectivity gradients for the task. In the second study, we analyze brain measured using high-density EEG during video watching, and perform an analysis using graph signal processing to estimate coupling and decoupling of source-localized electrophysiological activity on a functional connectivity graph. We discuss relationships between inter-subject correlation during video watching and structure-function decoupling at the individual level, and as a function of the underlying graph. The overarching goal of this line of work is to explore whether connectivity-informed analysis of brain activity can contribute to a better understanding of brain complexity as multimodal signals over networks."
},
{
"name" : "Fabrizio de Vico Fallani",
"last_name": "de Vico Fallani",
"affiliation": "Brain and Spine Institute (ICM) in Paris, France",
"title" : "Statistical models of complex brain networks",
"url" : "https://nicofarr.github.io/",
"image_src": "assets/img/fab3.png",
"field": "AI",
"date": "16.12.2022",
"day": "Friday",
"time": "1645",
"abstract": "The brain is a highly complex system. Disentangling the underlying network structure is crucial to understand the brain functioning under both healthy and pathological conditions. Yet, analyzing brain networks is challenging, in part because their structure represents only one possible realization of a generative stochastic process which is in general unknown. Having a formal way to cope with such intrinsic variability is therefore central for the characterization of brain network properties. Here, we focus on the recent advances of exponential random graph models (ERGMs), as a powerful means to identify the local connection mechanisms behind observed global network structure. Efforts are presented on the quest for basic organizational properties of human brain networks, as well as on the identification of predictive biomarkers of neurological diseases such as stroke. We conclude with a discussion on how emerging results and tools from statistical graph modeling could lead to a finer probabilistic description of complex systems in network neuroscience."
},
{
"name" : "Philippe Isope",
"last_name": "Isope",
"affiliation": "Institute of Cellular and Integrative Neuroscience, University of Strasbourg, France",
"title" : "Spatiotemporal information processing in the cerebellar cortex",
"url" : "https://inci-en.u-strasbg.fr/?page_id=432",
"image_src": "assets/img/isope.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Friday",
"time": "0900",
"abstract": "The cerebellum, which plays a major role in the control, timing and learning of skilled movements, is at the heart of motor coordination. To execute coordinated movements, the cerebellum processes multimodal contextual sensorimotor information conveyed by the mossy fiber inputs with internal models of the motor apparatus in order to predict sensory consequences of action. This prediction is necessary for delayed temporal associations between stimuli, sensory cancellation during voluntary movement for the detection of unexpected events, and fine tuning of motor and cognitive behaviors. A major goal is to understand how these temporal computations are performed in cerebellar circuits. I will first describe our work in rodents showing that cerebellar microcircuits encode internal models of locomotor adaptation. In a second part, I will introduce how short term plasticity control temporal processing in cerebellar modules."
},
{
"name" : "Thomas Miconi",
"last_name": "Miconi",
"affiliation": "ML Collective",
"title" : "Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning",
"url" : "https://scholar.harvard.edu/tmiconi",
"image_src": "assets/img/thomas_miconi.jpeg",
"field": "AI",
"date": "15.12.2022",
"day": "Thursday",
"time": "1645",
"abstract": "A hallmark of intelligence is the ability to autonomously learn new flexible, cognitive behaviors - that is, behaviors where the appropriate action depends not just on immediate stimuli (as in simple reflexive stimulus-response associations), but on memorized contextual information. Such cognitive, memory-dependent behaviors are by definition meta-learning (``learning to learn'') tasks. Artificial agents can learn a given cognitive task with external, human-designed meta-learning algorithms. By contrast, animals are able to pick up such cognitive tasks automatically, from stimuli and rewards alone, through the operation of their own  internal machinery: evolution has endowed animals with the ability to automatically acquire novel cognitive tasks, including tasks never seen during evolution. Can we harness this process to generate artificial agents with such abilities? Here we evolve neural networks, endowed with plastic connections and neuromodulation, over a sizable set of simple meta-learning tasks based on a framework from computational neuroscience. The resulting evolved networks can automatically modify their own connectivity to acquire a novel simple cognitive task, never seen during evolution, from stimuli and rewards alone, through the spontaneous operation of their evolved neural organization and plasticity system. We suggest that attending to the multiplicity of loops involved in natural learning may provide useful insight into the emergence of intelligent behavior."
},
{
    "name" : "Jie Mei",
    "last_name": "Mei",
    "affiliation": "IRCN Tokyo, Japan",
    "title" : "Neuromodulation-inspired mechanisms improve the performance of deep neural networks in a spatial learning task",
    "url" : "https://www.researchgate.net/profile/Jie-Mei-8",
    "image_src": "assets/img/jie_mei.jpeg",
    "field": "AI",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "1445",
    "abstract": "In recent years, the biology underlying adaptive learning has been modeled, leading to behavioral benefits in a variety of tasks. Studies have investigated how the neuromodulatory system, a major driver of synaptic plasticity, plays a role in training deep neural networks (DNNs). In this study, we explored the effects of neuromodulation on learning and single unit activities in a spatial learning task. Inspired by a previously proposed multi-scale neuromodulatory framework, plastic components, dropout modulation and learning rate decay were added to a DNN model. To explore the interplay between neuromodulation andersingle-unit activity patterns, we also studied properties of units in the fully-connected layer. As a result, we observed (1) behavioral benefits including faster model learning and smaller error of ambulation, and (2) altered activities of grid cell-like units in the fully-connected layer of the DNN upon integration of neuromodulatory components. We concluded that neuromodulatory components can affect learning trajectories, outcomes, and single unit activities, in a component- and hyperparameter-dependent manner. As one of the first studies that examined the role of neuromodulation in learning tasks using a biologically-realistic DNN, this study will encourage research that incorporates biological details, such as neuronal morphology and concurrent chemical neuromodulation processes, to neuromodulation-inspired models."
    }, 
    {
        "name" : "Emil Waernberg",
        "last_name": "Waernberg",
        "affiliation": "KTH and KI, Stockholm, Sweden",
        "title" : "Reinforcement learning in basal ganglia",
        "url" : "",
        "image_src": "assets/img/emil.jpg",
        "field": "Neuro",
        "date": "16.12.2022",
        "day": "Thursday",
        "time": "1100",
        "abstract": ""
        } 
    ]
