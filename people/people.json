[
{
    "name" : "Arvind Kumar",
    "last_name": "Kumar",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "",
    "url" : "https://www.kth.se/profile/arvindku",
    "image_src": "assets/img/arvindku.jpeg",
    "field": "Neuroscience",
    "date": "",
    "day": "",
    "time": "",
     "abstract": ""
},
{
    "name" : "Srikanth Ramaswamy",
    "last_name": "Ramaswamy",
    "affiliation": "New Castle University, UK",
    "title" : "",
    "url" : "https://people.epfl.ch/srikanth.ramaswamy",
    "image_src": "assets/img/Srikanth_Ramaswamy.jpeg",
    "field": "Neuroscience",
    "date": "",
    "day": "",
    "time": "",
     "abstract": ""
},
{
    "name" : "Saikat  Chatterjee",
    "last_name": "Chatterjee",
    "affiliation": "KTH Royal Institute of Technology, Stockholm",
    "title" : "",
    "url" : "https://www.kth.se/profile/sach",
    "image_src": "assets/img/Saikat_Chatterjee.png",
    "field": "AI",
    "date": "",
    "day": "",
    "time": "",
     "abstract": ""
},
{
    "name" : "Alain Destexhe",
    "last_name": "Destexhe",
    "affiliation": "CNRS, European Institute of Theoretical Neuroscience, Paris, France",
    "title" : "TBA",
    "url" : "https://neuropsi.cnrs.fr/annuaire/alain-destexhe/",
    "image_src": "assets/img/alain_destehxe.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "1600",
     "abstract": ""
},
{
    "name" : "Richard Naud",
    "last_name": "Naud",
    "affiliation": "University of Ottawa",
    "title" : "New biological solutions to the credit assignment problem",
    "url" : "https://med.uottawa.ca/cellular-molecular/people/naud-richard",
    "image_src": "assets/img/richard_naud.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "0945",
     "abstract": "Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well established that it depends on pre- and postsynaptic activity. How can such local learning give rise to meaningful decisions after being used by millions of neurons? Particularly, when the neurons receiving raw data are many synapses apart from the neurons making the decisions, how can the credit of a right/wrong decision be communicated to all the synapses that have taken a part in it? This is the credit assignment problem. AI has many algorithms to solve it, but these are impossible to implement by the brain as such. In this presentation, I am going to survey the recent theories and data that allowing us to understand how the brain coordinates plasticity by approximations of AI algorithms"
},    
{
    "name" : "Stefano Panzeri",
    "last_name": "Panzeri",
    "affiliation": "UKE Hamburg, Germany",
    "title" : "How activity correlations between neurons shape encoding and readout of sensory information",
    "url" : "https://www.uke.de/english/departments-institutes/institutes/department-of-excellence-for-neural-information-processing/team/index.html",
    "image_src": "assets/img/panzeri_stefano.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "1100",
     "abstract": "The collective activity of a population of neurons is critical for many brain functions. A fundamental question is how activity correlations between neurons affect how neural populations process information. We present a theory, built on the analyses of simultaneous recordings of activity of populations on neurons either in sensory or posterior parietal cortical areas, of how such correlations serve multiple functions performed by neural populations, including shaping the encoding of information in population codes, generating codes across multiple timescales, and facilitating information transmission to and readout by downstream brain areas to guide behavior. Here, we review this theory and we further present ideas on how to combine large-scale simultaneous recordings of neural populations, computational models, analyses of behavior, optogenetics, and anatomy to unravel how the structure of correlations might be optimized to serve multiple functions.     "
},
{
    "name" : "Upinder Bhalla",
    "last_name": "Bhalla",
    "affiliation": "National Center for Biological Sciences, Bangalore, India",
    "title" : "Seeing what matters: how single neurons extract patterns amidst noise",
    "url" : "https://www.ncbs.res.in/faculty/bhalla",
    "image_src": "assets/img/upi_bhalla.jpeg",
    "field": "Neural Coding",
    "date": "15.12.2022",
    "day": "Thursday",
    "time": "0900",
     "abstract": "Neurons are remarkably fast, efficient and versatile computing units. To first order, a single neuron can be simulated by a rather substantial neural network model, or by a large system of partial differential equations. What computations can the real neuron do with these complex dynamics? I'll discuss two examples, one like the prediction of where a cricket ball will fly, and the other related to the ''what was that?'' moment when we hear a  repeated sound amidst background noise. One of the uniquely powerful capabilities of neurons is their ability to extract signals from complex patterns in space and time, against a background of uncorrelated neural activity. The trajectory prediction problem can be framed as sequential pattern recognition. I'll discuss how sequential pattern recognition is implemented in neurons, and how this operation is equivalent to the functioning of a convolutional neural network. I'll then take on the ''what was that?'' problem of recognizing unique repeated inputs. I'll discuss some preliminary data which suggests that excitatory-inhibitory (EI) balance is asymmetric with respect to sequences of patterns. This means that certain spatiotemporal input sequences can 'escape' EI balance and cause the neuron to become more active. Overall, both these computations, take advantage of the intricate biophysics and biochemistry of the neuron to perform real-time pattern recognition with extraordinary energy efficiency."
},
{
"name" : "Vatsala Thirumalai",
"last_name": "Thirumalai",
"affiliation": "National Center for Biological Sciences, Bangalore, India",
"title" : "TBA",
"url" : "https://www.ncbs.res.in/faculty/vatsala",
"image_src": "assets/img/vatsala.jpeg",
"field": "Neuromodulation",
"date": "16.12.2022",
"day": "Friday",
"time": "0945",
 "abstract": ""
},
{
"name" : "Rishikesh Narayanan",
"last_name": "Narayanan",
"affiliation": "Indian Institute of Sciences, Bangalore, India",
"title" : "TBA",
"url" : "http://mbu.iisc.ac.in/~rngrp/",
"image_src": "assets/img/Rishi.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Thursday",
"time": "1445",
"abstract": ""
},
{
"name" : "Vijay Balasubramnium",
"last_name": "Balasubramnium",
"affiliation": "University of Pennsylvania, PA, USA",
"title" : "Occam's Razor in inference by humans and machines",
"url" : "https://live-sas-physics.pantheon.sas.upenn.edu/people/standing-faculty/vijay-balasubramanian",
"image_src": "assets/img/vijay_bala.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Thursday",
"time": "1400",
"abstract": "Occam's Razor is the principle that, all else being equal, simpler explanations for a set of observations should be preferred over complex ones.  This idea, derived from a famous dictum of the 13th century Scholastic philosopher William of Ockham, has a quantitative realization in modern mathematical theories of statistical model selection.   Experiments have also suggested that it plays a role in human perception and decision making.  I will describe the Bayesian theory of Occam's Razor, and discuss how it penalizes "complexity" understood in terms of the geometry of models seen as manifolds in the space of probability distributions.  Then I will present data from experiments with humans and artificial neural networks trained to perform decision tasks.  I will show that both humans and machines demonstrate an aversion to complex explanations, with simplicity biases quantitatively explained by the Bayesian theory.   These biases persist in humans, but not in machines, even in tasks where they are maladaptive and lower overall performance. Taken together, our results imply that principled notions of statistical model complexity have direct, quantitative relevance to human and machine decision-making. Our results also establish a new understanding of the computational foundations, and behavioral benefits, of our predilection for inferring simplicity in the latent properties of our complex world."
},
{
"name" : "Yasser Roudi",
"last_name": "Roudi",
"affiliation": "Kavli Institute for Systems Neuroscience, Trondheim, Norway",
"title" : "Quantifying relevance in learning and inference",
"url" : "https://www.ntnu.edu/employees/yasser.roudi",
"image_src": "assets/img/yasser_roudi.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Thursday",
"time": "1645",
"abstract": "Learning is a distinctive feature of intelligent behaviour. High-throughput experimental data and Big Data promise to open new windows on complex systems such as cells, the brain or our societies. Yet, the puzzling success of Artificial Intelligence and Machine Learning shows that we still have a poor conceptual understanding of learning. These applications push statistical inference into uncharted territories where data is highdimensional and scarce, and prior information on 'true' models is scant if not totally absent. Here we review recent progress on understanding learning, based on the notion of 'relevance'. The relevance, as we define it here, quantifies the amount of information that a dataset or the internal representation of a learning machine contains on the generative model of the data. This allows us to define maximally informative samples, on one hand, and optimal learning machines on the other. These are ideal limits of samples and of machines, that contain the maximal amount of information about the unknown generative process, at a given resolution (or level of compression). Both ideal limits exhibit critical features in the statistical sense: Maximally informative samples are characterised by a power-law frequency distribution (statistical criticality) and optimal learning machines by an anomalously large susceptibility. The trade-off between resolution (i.e. compression) and relevance distinguishes the regime of noisy representations from that of lossy compression. These are separated by a special point characterised by Zipf’s law statistics. This identifies samples obeying Zipf’s law as the most compressed loss-less representations that are optimal in the sense of maximal relevance. Criticality in optimal learning machines manifests in an exponential degeneracy of energy levels, that leads to unusual thermodynamic properties. This distinctive feature is consistent with the invariance of the classification under coarse graining of the output, which is a desirable property of learning machines. This theoretical framework is corroborated by empirical analysis showing (i) how the concept of relevance can be useful to identify relevant variables in high-dimensional inference and (ii) that widely used machine learning architectures approach reasonably well the ideal limit of optimal learning machines, within the limits of the data with which they are trained."
},
{
"name" : "Roshan Cools",
"last_name": "Cools",
"affiliation": "Donders Institute for Brain, Cognition and Behaviour, Centre for Cognitive Neuroimaging. The Netherlands",
"title" : "Chemistry of the adaptive mind. Lessons from dopamine?",
"url" : "http://www.roshancools.com/",
"image_src": "assets/img/roshan_cools.jpg",
"field": "Neuromodulation",
"date": "16.12.2022",
"day": "Friday",
"time": "0900",
"abstract": "The human brain faces a variety of computational dilemmas, including the flexibility/stability, the speed/accuracy and the labor/leisure tradeoff. Given its role in reinforcement learning and motivation, striatal dopamine is particularly well suited to dynamically regulate these computational tradeoffs depending on constantly changing task demands. To illustrate this, I will discuss evidence from studies on learning, motivation and cognitive control in human volunteers, using chemical PET imaging, psychopharmacology, and/or fMRI. These studies also begin to elucidate the mechanisms underlying the huge variability in dopaminergic drug effects across different individuals and across different task contexts"
},
{
"name" : "Dinos Meletis",
"last_name": "Meletis",
"affiliation": "Karolinska Institute, Sweden",
"title" : "TBA",
"url" : "https://ki.se/en/neuro/meletis-laboratory",
"image_src": "assets/img/Dinos_Meletis.jpeg",
"field": "Neuromodulation",
"date": "16.12.2022",
"day": "Friday",
"time": "1145",
"abstract": ""
},
{
"name" : "Katharina Glomb",
"last_name": "Glomb",
"affiliation": "Berlin Institute of Health at Charité (BIH)",
"title" : "Towards quality control for harmonic modes",
"url" : "https://wp.unil.ch/connectomics/katharina-glomb/",
"image_src": "assets/img/Katharina_Glomb.jpeg",
"field": "Graphs",
"date": "16.12.2022",
"day": "Friday",
"time": "1400",
"abstract": "Basic graph signal processing (GSP) techniques can be used to transform time series obtained through neuroimaging (be it M/EEG, fMRI, or others) into the graph spectral domain. The necessary basis functions - or 'harmonic modes' in network neuroscience - are obtained from functional or structural connectivity matrices that encode network organization in the brain. This procedure involves a number of user-defined parameter choices, namely the type of Laplacian, the number of nearest neighbors in the graph, and the number of dimensions of the reduced space, all of which have an impact on the properties of the harmonic modes. If one applies GSP for the purposes of dimensionality reduction, it is relatively easy to choose these parameters, as one wants to achieve optimal compression. However, it is less clear what properties harmonic modes should have to meaningfully reflect the underlying brain connectivity. Quantitative criteria as to how to set these parameters - or indeed a systematic understanding of how parameter choices impact harmonic modes - are missing thus far, posing a serious limitation for interpreting harmonic modes in terms of their functional significance. In my talk, I will address recent ideas of how to tackle this limitation, looking at embedding and reconstruction performance as well as discussing graph properties that can be derived from the harmonic modes themselves. "
},
{
"name" : "Maria Giulia Preti",
"last_name": "Preti",
"affiliation": "Center for Biomedical Imaging, EPFL, Switzerland",
"title" : "Graph signal processing for neurogimaging to identify unique structure-function coupling signatures ",
"url" : "https://people.epfl.ch/maria.preti/?lang=en",
"image_src": "assets/img/maria_preti.jpeg",
"field": "Signal Processing",
"date": "16.12.2022",
"day": "Friday",
"time": "1445",
"abstract": "The relation between functional activity and the underlying neural architecture in the brain is complex and remains largely unexplored. In this context, graph signal processing (GSP) represents a novel framework allowing to link functional activity signals, e.g., from functional magnetic resonance imaging (fMRI), and the structural connections underneath (assessed with diffusion MRI) in a non-trivial way. Functional activity patterns are represented as graph signals, defined on top of the structural connectome graph, and graph spectral filtering can be used to distinguish the components of the functional activity that are more or less smooth on the graph; i.e., coupled or decoupled from brain structure, respectively. Here, we investigate and compare structure-function coupling assessed at different temporal scales and with different modalities; i.e., resting-state fMRI and magnetoencephalography (MEG). In particular, we investigate the capability of structure-function coupling to identify individuals, representing a unique signature of their brains. We conclude that structure-function coupling performs very accurately for individual fingerprinting, with different brain patterns contributing at different time-scales."
},
{
"name" : "Nicolas Farrugia",
"last_name": "Farrugia",
"affiliation": " IMT Atlantique, France",
"title" : "Interpreting brain activity through connectivity using graph signal processing",
"url" : "https://nicofarr.github.io/",
"image_src": "assets/img/nico_juin.jpeg",
"field": "AI",
"date": "16.12.2022",
"day": "Friday",
"time": "1600",
"abstract": "The application of graph theory to model the complex structure and function of the brain has shed new light on its organization, prompting the emergence of network neuroscience. Despite the tremendous progress that has been achieved in this field, still relatively few methods exploit the topology of brain networks to analyze brain activity. Recent attempts in this direction have leveraged on the one hand graph spectral analysis (to decompose brain connectivity into eigenmodes or gradients) and graph signal processing (to decompose brain activity “coupled to” an underlying network in graph Fourier modes). In this talk, we will describe two ongoing works that attempt at integrating knowledge from brain connectivity in order to decode and interpret brain activity. In the first contribution, we use functional connectivity graphs to define spectral convolution operators in a deep residual network trained on task decoding. We show how parameter pruning can be used to select the most important connectivity gradients for the task. In the second study, we analyze brain measured using high-density EEG during video watching, and perform an analysis using graph signal processing to estimate coupling and decoupling of source-localized electrophysiological activity on a functional connectivity graph. We discuss relationships between inter-subject correlation during video watching and structure-function decoupling at the individual level, and as a function of the underlying graph. The overarching goal of this line of work is to explore whether connectivity-informed analysis of brain activity can contribute to a better understanding of brain complexity as multimodal signals over networks."
},
{
"name" : "Fabrizio de Vico Fallani",
"last_name": "de Vico Fallani",
"affiliation": "Brain and Spine Institute (ICM) in Paris, France",
"title" : "Statistical models of complex brain networks",
"url" : "https://nicofarr.github.io/",
"image_src": "assets/img/fab3.png",
"field": "AI",
"date": "16.12.2022",
"day": "Friday",
"time": "1645",
"abstract": "The brain is a highly complex system. Disentangling the underlying network structure is crucial to understand the brain functioning under both healthy and pathological conditions. Yet, analyzing brain networks is challenging, in part because their structure represents only one possible realization of a generative stochastic process which is in general unknown. Having a formal way to cope with such intrinsic variability is therefore central for the characterization of brain network properties. Here, we focus on the recent advances of exponential random graph models (ERGMs), as a powerful means to identify the local connection mechanisms behind observed global network structure. Efforts are presented on the quest for basic organizational properties of human brain networks, as well as on the identification of predictive biomarkers of neurological diseases such as stroke. We conclude with a discussion on how emerging results and tools from statistical graph modeling could lead to a finer probabilistic description of complex systems in network neuroscience."
},
{
"name" : "Philippe Isope",
"last_name": "Isope",
"affiliation": "Institute of Cellular and Integrative Neuroscience, University of Strasbourg, France",
"title" : "Spatiotemporal informmation processing in the cerebellar cortex",
"url" : "https://inci-en.u-strasbg.fr/?page_id=432",
"image_src": "assets/img/isope.jpeg",
"field": "Neural Coding",
"date": "15.12.2022",
"day": "Thursday",
"time": "1145",
"abstract": ""
},
{
"name" : "Thomas Micconi",
"last_name": "Micconi",
"affiliation": "ML Collective",
"title" : "Learning to acquire novel cognitive tasks with evolution, plasticity and meta-meta-learning",
"url" : "https://scholar.harvard.edu/tmiconi",
"image_src": "assets/img/thomas_miconi.jpeg",
"field": "AI",
"date": "15.12.2022",
"day": "Thursday",
"time": "1730",
"abstract": "A hallmark of intelligence is the ability to autonomously learn new flexible, cognitive behaviors - that is, behaviors where the appropriate action depends not just on immediate stimuli (as in simple reflexive stimulus-response associations), but on memorized contextual information. Such cognitive, memory-dependent behaviors are by definition meta-learning (``learning to learn'') tasks. Artificial agents can learn a given cognitive task with external, human-designed meta-learning algorithms. By contrast, animals are able to pick up such cognitive tasks automatically, from stimuli and rewards alone, through the operation of their own  internal machinery: evolution has endowed animals with the ability to automatically acquire novel cognitive tasks, including tasks never seen during evolution. Can we harness this process to generate artificial agents with such abilities? Here we evolve neural networks, endowed with plastic connections and neuromodulation, over a sizable set of simple meta-learning tasks based on a framework from computational neuroscience. The resulting evolved networks can automatically modify their own connectivity to acquire a novel simple cognitive task, never seen during evolution, from stimuli and rewards alone, through the spontaneous operation of their evolved neural organization and plasticity system. We suggest that attending to the multiplicity of loops involved in natural learning may provide useful insight into the emergence of intelligent behavior."
} 
]
    
